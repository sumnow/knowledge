# http connection and rpc

### http 为什么是无状态的

首先, 要定义一下什么叫【无状态】. 假设用户A向服务B发了一个请求1, 再次发送一个请求2. 服务端本身完全不知道两个请求来自同一个用户, 这在协议层次就是【无状态】的. 

【无状态】设计不是因为 @方正 所说的历史原因, 而是故意为之. 无状态就意味着服务端可以根据需要将请求分发到集群的任何一个节点, 对缓存、 负载均衡有明显的好处, 这一点很容易找到相关文献. 

很多人对【无状态】感到不理解, 大部分情况是误解了【无状态】的含义. http【无状态】仅仅是在*协议层*, 当业务需要状态的时候, 可以通过request中数据携带所需状态的id来实现. 例如, 为了让服务器知道是同一个用户的请求, 请求1和请求2中必须携带一个相同的id, 让服务端可以根据这个id, 最终找到用户数据(【状态】). 

实现1: 这个状态如果放在处理请求的服务器进程中(例如session), 那服务器进程就是有状态的, 该用户下一个请求如果没分发到这个进程, 就会拿不到上一次请求留下的状态, 这样会影响负载均衡和缓存的实现. 

实现2: 这个状态如果放在处理请求的服务器进程之外的集中式存储, 那服务器进程仍然是无状态的, 可以集群、 负载均衡. 无状态服务一般都用这种方案. 

最后我的观点是: 无状态和有状态服务适合不同的场景, 并没有绝对的优劣. 

### HTTP连接方式的进化史

##### HTTP/0.9时代: 短连接

每个HTTP请求都要经历一次DNS解析、 三次握手、 传输和四次挥手. 反复创建和断开TCP连接的开销巨大, 在现在看来, 这种传输方式简直是糟糕透顶. 

##### HTTP/1.0时代: 持久连接概念提出

人们认识到短连接的弊端, 提出了持久连接的概念, 在HTTP/1.0中得到了初步的支持. 
持久连接, 即一个TCP连接服务多次请求: 
客户端在请求header中携带Connection:
Keep-Alive, 即是在向服务端请求持久连接. 如果服务端接受持久连接, 则会在响应header中同样携带Connection: 
Keep-Alive, 这样客户端便会继续使用同一个TCP连接发送接下来的若干请求. (Keep-Alive的默认参数是[timout=5, 
max=100], 即一个TCP连接可以服务至多5秒内的100次请求)

当服务端主动切断一个持久连接时(或服务端不支持持久连接), 则会在header中携带Connection: Close, 要求客户端停止使用这一连接. 

##### HTTP/1.1时代: 持久连接成为默认的连接方式; 提出pipelining概念

HTTP/1.1开始, 即使请求header中没有携带Connection: Keep-Alive, 传输也会默认以持久连接的方式进行. 

目前所有的浏览器都默认请求持久连接, 几乎所有的HTTP服务端也都默认开启对持久连接的支持, 短连接正式成为过去式. (HTTP/1.1的发布时间是1997年, 最后一次对协议的补充是在1999年, 我们可以夸张地说: HTTP短连接这个概念已经过时了近20年了. )

同时, 持久连接的弊端被提出 —— HOLB(Head of Line Blocking)

即持久连接下一个连接中的请求仍然是串行的, 如果某个请求出现网络阻塞等问题, 会导致同一条连接上的后续请求被阻塞. 

所以HTTP/1.1中提出了pipelining概念, 即客户端可以在一个请求发送完成后不等待响应便直接发起第二个请求, 服务端在返回响应时会按请求到达的顺序依次返回, 这样就极大地降低了延迟. 

然而pipelining并没有彻底解决HOLB, 为了让同一个连接中的多个响应能够和多个请求匹配上, 响应仍然是按请求的顺序串行返回的. 所以pipelining并没有被广泛接受, 几乎所有代理服务都不支持pipelining, 部分浏览器不支持pipelining, 支持的大部分也会将其默认关闭. 

##### SPDY和HTTP/2: multiplexing

multiplexing即多路复用, 在SPDY中提出, 同时也在HTTP/2中实现. 

multiplexing技术能够让多个请求和响应的传输完全混杂在一起进行, 通过streamId来互相区别. 这彻底解决了holb问题, 同时还允许给每个请求设置优先级, 服务端会先响应优先级高的请求. 

现在Chrome、 FireFox、 Opera、 IE、 Safari的最新版本都支持SPDY, Nginx/Apache HTTPD/Jetty/Tomcat等服务端也都提供了对SPDY的支持.

另外, 谷歌已经关闭SPDY项目, 正式为HTTP/2让路. 可以认为SPDY是HTTP/2的前身和探路者. 

### 既然有 HTTP 请求, 为什么还要用 RPC 调用? 

这个回答里恰巧讲了一些rpc通信协议的细节, 但是强调一遍通信协议不是rpc最重要的部分, 不要被这个回答带偏了. 如果要了解rpc请更多的去了解服务治理(soa)的一些基本策略, 推荐去看看dubbo的文档. 

这个问题其实是有理解误区的, 首先 http 和 rpc 并不是一个并行概念. 

rpc是远端过程调用, 其调用协议通常包含传输协议和序列化协议. 

传输协议包含: 如著名的 [gRPC](grpc / grpc.io) 使用的 http2 协议, 也有如dubbo一类的自定义报文的tcp协议. 

序列化协议包含: 如基于文本编码的 xml json, 也有二进制编码的 protobuf hessian等. 

因此我理解的你想问的问题应该是: 为什么要使用自定义 tcp 协议的 rpc 做后端进程通信? 

要解决这个问题就应该搞清楚 http 使用的 tcp 协议, 和我们自定义的 tcp 协议在报文上的区别. 

首先要否认一点 http 协议相较于自定义tcp报文协议, 增加的开销在于连接的建立与断开. http协议是支持连接池复用的, 也就是建立一定数量的连接不断开, 并不会频繁的创建和销毁连接. 二一要说的是http也可以使用protobuf这种二进制编码协议对内容进行编码, 因此二者最大的区别还是在传输协议上. 

通用定义的http1.1协议的tcp报文包含太多废信息, 一个POST协议的格式大致如下

``` http

HTTP/1.0 200 OK 
Content-Type: text/plain
Content-Length: 137582
Expires: Thu, 05 Dec 1997 16:00:00 GMT
Last-Modified: Wed, 5 August 1996 15:55:28 GMT
Server: Apache 0.84
<html>
  <body>Hello World</body>
</html>
```

即使编码协议也就是body是使用二进制编码协议, 报文元数据也就是header头的键值对却用了文本编码, 非常占字节数. 如上图所使用的报文中有效字节数仅仅占约 30%, 也就是70%的时间用于传输元数据废编码. 当然实际情况下报文内容可能会比这个长, 但是报头所占的比例也是非常可观的. 

那么假如我们使用自定义tcp协议的报文如下

报头占用的字节数也就只有16个byte, 极大地精简了传输内容. 

这也就是为什么后端进程间通常会采用自定义tcp协议的rpc来进行通信的原因. 

-- 分割线 2017.08.03 --

可能回答里面没有描述清楚, 这个回答仅仅是用来纠正victory的回答的. 所谓的效率优势是针对http1.1协议来讲的, http2.0协议已经优化编码效率问题, 像grpc这种rpc库使用的就是http2.0协议. 这么来说吧http容器的性能测试单位通常是kqps, 自定义tpc协议则通常是以10kqps到100kqps为基准

简单来说成熟的rpc库相对http容器, 更多的是封装了"服务发现", "负载均衡", "熔断降级"一类面向服务的高级特性. 可以这么理解, rpc框架是面向服务的更高级的封装. 如果把一个http servlet容器上封装一层服务发现和函数代理调用, 那它就已经可以做一个rpc框架了. 

所以为什么要用rpc调用? 

因为良好的rpc调用是面向服务的封装, 针对服务的可用性和效率等都做了优化. 单纯使用http调用则缺少了这些特性. 
