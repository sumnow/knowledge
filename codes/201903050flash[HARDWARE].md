# flash

原因一: 距离不同

距离不是主要因素, 但是最好懂, 所以放在最前面说. 内存离CPU比较远, 所以要耗费更长的时间读取.

以3GHz的CPU为例, 电流每秒钟可以振荡30亿次, 每次耗时大约为0.33纳秒. 光在1纳秒的时间内, 可以前进30厘米. 也就是说, 在CPU的一个时钟周期内, 光可以前进10厘米. 因此, 如果内存距离CPU超过5厘米, 就不可能在一个时钟周期内完成数据的读取, 这还没有考虑硬件的限制和电流实际上达不到光速. 相比之下, 寄存器在CPU内部, 当然读起来会快一点.

距离对于桌面电脑影响很大, 对于手机影响就要小得多. 手机CPU的时钟频率比较慢(iPhone 5s为1.3GHz), 而且手机的内存紧挨着CPU.

原因二: 硬件设计不同

苹果公司新推出的iPhone 5s, CPU是A7, 寄存器有6000多位(31个64位寄存器, 加上32个128位寄存器). 而iPhone 5s的内存是1GB, 约为80亿位(bit). 这意味着, 高性能、高成本、高耗电的设计可以用在寄存器上, 反正只有6000多位, 而不能用在内存上. 因为每个位的成本和能耗只要增加一点点, 就会被放大80亿倍.

事实上确实如此, 内存的设计相对简单, 每个位就是一个电容和一个晶体管, 而寄存器的设计则完全不同, 多出好几个电子元件. 并且通电以后, 寄存器的晶体管一直有电, 而内存的晶体管只有用到的才有电, 没用到的就没电, 这样有利于省电. 这些设计上的因素, 决定了寄存器比内存读取速度更快.

原因三: 工作方式不同

寄存器的工作方式很简单, 只有两步:(1)找到相关的位, (2)读取这些位.

内存的工作方式就要复杂得多:

(1)找到数据的指针.(指针可能存放在寄存器内, 所以这一步就已经包括寄存器的全部工作了.)

(2)将指针送往内存管理单元(MMU), 由MMU将虚拟的内存地址翻译成实际的物理地址.

(3)将物理地址送往内存控制器(memory controller), 由内存控制器找出该地址在哪一根内存插槽(bank)上.

(4)确定数据在哪一个内存块(chunk)上, 从该块读取数据.

(5)数据先送回内存控制器, 再送回CPU, 然后开始使用.

内存的工作流程比寄存器多出许多步. 每一步都会产生延迟, 累积起来就使得内存比寄存器慢得多.

为了缓解寄存器与内存之间的巨大速度差异, 硬件设计师做出了许多努力, 包括在CPU内部设置缓存、优化CPU工作方式, 尽量一次性从内存读取指令所要用到的全部数据等等.

### dram和sram

上次去wiki搜 flash memory, 顺着搜了DRAM和CPU, 可以分的很清晰.

寄存器由好多个晶体管组成, 成本高, 和CPU同频好象(速度就在这里体现出来了)可以直接在寻址, 而内存RAM之类电路简单, 每个bit一个管, 由电容存储数据, 需要专用电路来不停的刷新电容保持数据不丢, 也需要专门的寻址电路接口什么的, 频率好象是和主板一样(板频是cpu的几分之一, 慢), 读写也要好多个指令周期.

基本上就是这样, 可以自己去wiki.

最好玩的是关于flash, 就是我们用的U盘和固态硬盘, 原来用浮闸Mos管做的, 把电荷永久封在闸里, 根据每个管能存的bit数不同还分为SLC, MLC, TLC, TLC是一管有3个状态电位, 当然可靠性就更差

现代计算机, 虽然性能很高, 但是和上世纪7、8十年代的计算机比, 其实结构都差不多. 现在讲存储, 一般讲有内存和外存, 内存一般有寄存器(register), 缓存(cache)和内存(memory), 有些小型应用例如MCU没有cache, 甚至没有memory——直接从flash/ROM到register. 寄存器是CPU基础单元, CPU直接处理的内存就它了, 好比医院, 医生对面的椅子就是寄存器, 要看病的病人(data)就座这个椅子(register); 已经挂号的(data)进入诊室(cache)排队, 其他的就在医院里(memory). 医生可以操作的就是面对面的病人, 其他人要看病(如急病)也需先坐上这个位置, 这是最快的. 诊室里的座位相对于cache, 一般cache都是sram存储器, 速度很快, 但一般cpu不会直接访问, 而是要把数据挪到register后才可直接操作, 而一般的内存为DRAM, 速度比SRAM慢多了, 而且通过总线访问, 速度就更慢了.
应该说这不完全是距离问题, 而是内存结构和总线结构和访问方式的问题.

主要是SRAM和DRAM的差异, DRAM为了节约晶体管, 用了电容器维持电位, 但电容器会漏电, 需要周期性刷新, 刷新期间不能读写

